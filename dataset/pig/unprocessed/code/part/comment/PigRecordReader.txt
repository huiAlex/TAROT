/**
 * A wrapper around the actual RecordReader and loadfunc - this is needed for
 * two reasons
 * 1) To intercept the initialize call from hadoop and initialize the underlying
 * actual RecordReader with the right Context object - this is achieved by
 * looking up the Context corresponding to the input split this Reader is
 * supposed to process
 * 2) We need to give hadoop consistent key-value types - text and tuple
 * respectively - so PigRecordReader will call underlying Loader's getNext() to
 * get the Tuple value - the key is null text since key is not used in input to
 * map() in Pig.
 */

/**
     * the current Tuple value as returned by underlying
     * {@link LoadFunc#getNext()}
     */

/**
     * the Configuration object with data specific to the input the underlying
     * RecordReader will process (this is obtained after a
     * {@link LoadFunc#setLocation(String, org.apache.hadoop.mapreduce.Job)}
     * call and hence can contain specific properties the underlying
     * {@link InputFormat} might have put in.
     */

/**
     * @param context
     *
     */

/**
     * Get the record reader for the next chunk in this CombineFileSplit.
     */

