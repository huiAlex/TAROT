<Class>
    <Id>61</Id>
    <Package>org.apache.pig.backend.hadoop.executionengine.spark.running</Package>
    <ClassName>PigInputFormatSpark</ClassName>
    <SuperClass>PigInputFormat</SuperClass>
    <SuperInterfaceList>
        <SuperInterface></SuperInterface>
    </SuperInterfaceList>
    <ClassComment></ClassComment>
    <FieldList/>
    <MethodList>
        <Method>
            <MethodName>createRecordReader</MethodName>
            <MethodComment></MethodComment>
            <ReturnType>Text</ReturnType>
            <ParameterList>
                <Parameter>
                    <ParamName>split</ParamName>
                    <ParamType>InputSplit</ParamType>
                </Parameter>
                <Parameter>
                    <ParamName>context</ParamName>
                    <ParamType>TaskAttemptContext</ParamType>
                </Parameter>
            </ParameterList>
            <InnerVarList>
                <InnerVar>PigSplit [pigSplit=((SparkPigSplit)split).getWrappedPigSplit()]</InnerVar>
                <InnerVar>Configuration [conf=context.getConfiguration()]</InnerVar>
                <InnerVar>SparkRecordReaderFactory [sparkRecordReaderFactory=new SparkRecordReaderFactory(pigSplit,context)]</InnerVar>
            </InnerVarList>
            <InnerMethodInvokeList>
                <InnerMethodInvoke>null;resetUDFContext;[]</InnerMethodInvoke>
                <InnerMethodInvoke>pigSplit;setConf;[conf]</InnerMethodInvoke>
                <InnerMethodInvoke>PigMapReduce.sJobContext.getConfiguration();setInt;[PigImplConstants.PIG_SPLIT_INDEX, pigSplit.getSplitIndex()]</InnerMethodInvoke>
                <InnerMethodInvoke>null;initialize;[conf]</InnerMethodInvoke>
            </InnerMethodInvokeList>
            <ThrowExceptionList>
                <ExceptionType>IOException</ExceptionType>
                <ExceptionType>InterruptedException</ExceptionType>
            </ThrowExceptionList>
        </Method>
        <Method>
            <MethodName>getSplits</MethodName>
            <MethodComment>/** 
 * This is where we have to wrap PigSplits into SparkPigSplits
 * @param jobcontext
 * @return
 * @throws IOException
 * @throws InterruptedException
 */
</MethodComment>
            <ReturnType>InputSplit</ReturnType>
            <ParameterList>
                <Parameter>
                    <ParamName>jobcontext</ParamName>
                    <ParamType>JobContext</ParamType>
                </Parameter>
            </ParameterList>
            <InnerVarList>
                <InnerVar>List&lt;InputSplit&gt; [sparkPigSplits=new ArrayList&lt;&gt;()]</InnerVar>
                <InnerVar>List&lt;InputSplit&gt; [originalSplits=super.getSplits(jobcontext)]</InnerVar>
                <InnerVar>boolean [isFileSplits=true]</InnerVar>
            </InnerVarList>
            <InnerMethodInvokeList>
                <InnerMethodInvoke></InnerMethodInvoke>
            </InnerMethodInvokeList>
            <ThrowExceptionList>
                <ExceptionType>IOException</ExceptionType>
                <ExceptionType>InterruptedException</ExceptionType>
            </ThrowExceptionList>
        </Method>
        <Method>
            <MethodName>initialize</MethodName>
            <MethodComment></MethodComment>
            <ReturnType>void</ReturnType>
            <ParameterList>
                <Parameter>
                    <ParamName>jobConf</ParamName>
                    <ParamType>Configuration</ParamType>
                </Parameter>
            </ParameterList>
            <InnerVarList>
                <InnerVar>PigContext [pc=(PigContext)ObjectSerializer.deserialize(jobConf.get("pig.pigContext"))]</InnerVar>
                <InnerVar>PigHadoopLogger [pigHadoopLogger=PigHadoopLogger.getInstance()]</InnerVar>
            </InnerVarList>
            <InnerMethodInvokeList>
                <InnerMethodInvoke>MapRedUtil;setupUDFContext;[jobConf]</InnerMethodInvoke>
                <InnerMethodInvoke>SchemaTupleBackend;initialize;[jobConf, pc]</InnerMethodInvoke>
                <InnerMethodInvoke>PigMapReduce.sJobConfInternal;set;[jobConf]</InnerMethodInvoke>
                <InnerMethodInvoke>pigHadoopLogger;setAggregate;["true".equalsIgnoreCase(jobConf.get("aggregate.warning"))]</InnerMethodInvoke>
                <InnerMethodInvoke>pigHadoopLogger;setReporter;[(SparkCounters)ObjectSerializer.deserialize(jobConf.get("pig.spark.counters"))]</InnerMethodInvoke>
                <InnerMethodInvoke>PhysicalOperator;setPigLogger;[pigHadoopLogger]</InnerMethodInvoke>
            </InnerMethodInvokeList>
            <ThrowExceptionList>
                <ExceptionType>IOException</ExceptionType>
            </ThrowExceptionList>
        </Method>
        <Method>
            <MethodName>resetUDFContext</MethodName>
            <MethodComment></MethodComment>
            <ReturnType>void</ReturnType>
            <ParameterList/>
            <InnerVarList>
                <InnerVar></InnerVar>
            </InnerVarList>
            <InnerMethodInvokeList>
                <InnerMethodInvoke>UDFContext.getUDFContext();reset;[]</InnerMethodInvoke>
            </InnerMethodInvokeList>
            <ThrowExceptionList>
                <ExceptionType></ExceptionType>
            </ThrowExceptionList>
        </Method>
        <Method>
            <MethodName>SparkRecordReaderFactory</MethodName>
            <MethodComment></MethodComment>
            <ReturnType></ReturnType>
            <ParameterList>
                <Parameter>
                    <ParamName>split</ParamName>
                    <ParamType>InputSplit</ParamType>
                </Parameter>
                <Parameter>
                    <ParamName>context</ParamName>
                    <ParamType>TaskAttemptContext</ParamType>
                </Parameter>
            </ParameterList>
            <InnerVarList>
                <InnerVar></InnerVar>
            </InnerVarList>
            <InnerMethodInvokeList>
                <InnerMethodInvoke></InnerMethodInvoke>
            </InnerMethodInvokeList>
            <ThrowExceptionList>
                <ExceptionType>IOException</ExceptionType>
            </ThrowExceptionList>
        </Method>
        <Method>
            <MethodName>createRecordReader</MethodName>
            <MethodComment></MethodComment>
            <ReturnType>Text</ReturnType>
            <ParameterList/>
            <InnerVarList>
                <InnerVar></InnerVar>
            </InnerVarList>
            <InnerMethodInvokeList>
                <InnerMethodInvoke></InnerMethodInvoke>
            </InnerMethodInvokeList>
            <ThrowExceptionList>
                <ExceptionType>IOException</ExceptionType>
                <ExceptionType>InterruptedException</ExceptionType>
            </ThrowExceptionList>
        </Method>
    </MethodList>
</Class>