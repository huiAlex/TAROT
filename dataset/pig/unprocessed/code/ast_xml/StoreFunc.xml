<Class>
    <Id>237</Id>
    <Package>org.apache.pig</Package>
    <ClassName>StoreFunc</ClassName>
    <SuperClass></SuperClass>
    <SuperInterfaceList>
        <SuperInterface>StoreFuncInterface</SuperInterface>
    </SuperInterfaceList>
    <ClassComment>StoreFunc  /** 
 * StoreFuncs take records from Pig's processing and store them into a data store.  Most frequently this is an HDFS file, but it could also be an HBase instance, RDBMS, etc.
 */
</ClassComment>
    <FieldList/>
    <MethodList>
        <Method>
            <MethodName>relToAbsPathForStoreLocation</MethodName>
            <MethodComment>/** 
 * This method is called by the Pig runtime in the front end to convert the output location to an absolute path if the location is relative. The StoreFunc implementation is free to choose how it converts a relative location to an absolute location since this may depend on what the location string represent (hdfs path or some other data source).
 * @param location location as provided in the "store" statement of the script
 * @param curDir the current working direction based on any "cd" statementsin the script before the "store" statement. If there are no "cd" statements in the script, this would be the home directory - &lt;pre&gt;/user/&lt;username&gt; &lt;/pre&gt;
 * @return the absolute location based on the arguments passed
 * @throws IOException if the conversion is not possible
 */
</MethodComment>
            <ReturnType>String</ReturnType>
            <ParameterList>
                <Parameter>
                    <ParamName>location</ParamName>
                    <ParamType>String</ParamType>
                </Parameter>
                <Parameter>
                    <ParamName>curDir</ParamName>
                    <ParamType>Path</ParamType>
                </Parameter>
            </ParameterList>
            <InnerVarList>
                <InnerVar></InnerVar>
            </InnerVarList>
            <InnerMethodInvokeList>
                <InnerMethodInvoke></InnerMethodInvoke>
            </InnerMethodInvokeList>
            <ThrowExceptionList>
                <ExceptionType>IOException</ExceptionType>
            </ThrowExceptionList>
        </Method>
        <Method>
            <MethodName>getOutputFormat</MethodName>
            <MethodComment>/** 
 * Return the OutputFormat associated with StoreFunc.  This will be called on the front end during planning and on the backend during execution.
 * @return the {@link OutputFormat} associated with StoreFunc
 * @throws IOException if an exception occurs while constructing theOutputFormat
 */
</MethodComment>
            <ReturnType>OutputFormat</ReturnType>
            <ParameterList/>
            <ThrowExceptionList>
                <ExceptionType>IOException</ExceptionType>
            </ThrowExceptionList>
        </Method>
        <Method>
            <MethodName>setStoreLocation</MethodName>
            <MethodComment>/** 
 * Communicate to the storer the location where the data needs to be stored. The location string passed to the  {@link StoreFunc} here is thereturn value of  {@link StoreFunc#relToAbsPathForStoreLocation(String,Path)}This method will be called in the frontend and backend multiple times. Implementations should bear in mind that this method is called multiple times and should ensure there are no inconsistent side effects due to the multiple calls. {@link #checkSchema(ResourceSchema)} will be called before any call to{@link #setStoreLocation(String,Job)}.
 * @param location Location returned by{@link StoreFunc#relToAbsPathForStoreLocation(String,Path)}
 * @param job The {@link Job} object
 * @throws IOException if the location is not valid.
 */
</MethodComment>
            <ReturnType>void</ReturnType>
            <ParameterList>
                <Parameter>
                    <ParamName>location</ParamName>
                    <ParamType>String</ParamType>
                </Parameter>
                <Parameter>
                    <ParamName>job</ParamName>
                    <ParamType>Job</ParamType>
                </Parameter>
            </ParameterList>
            <ThrowExceptionList>
                <ExceptionType>IOException</ExceptionType>
            </ThrowExceptionList>
        </Method>
        <Method>
            <MethodName>checkSchema</MethodName>
            <MethodComment>/** 
 * Set the schema for data to be stored.  This will be called on the front end during planning if the store is associated with a schema. A Store function should implement this function to check that a given schema is acceptable to it.  For example, it can check that the correct partition keys are included; a storage function to be written directly to an OutputFormat can make sure the schema will translate in a well defined way.  Default implementation is a no-op.
 * @param s to be checked
 * @throws IOException if this schema is not acceptable.  It should includea detailed error message indicating what is wrong with the schema.
 */
</MethodComment>
            <ReturnType>void</ReturnType>
            <ParameterList>
                <Parameter>
                    <ParamName>s</ParamName>
                    <ParamType>ResourceSchema</ParamType>
                </Parameter>
            </ParameterList>
            <InnerVarList>
                <InnerVar></InnerVar>
            </InnerVarList>
            <InnerMethodInvokeList>
                <InnerMethodInvoke></InnerMethodInvoke>
            </InnerMethodInvokeList>
            <ThrowExceptionList>
                <ExceptionType>IOException</ExceptionType>
            </ThrowExceptionList>
        </Method>
        <Method>
            <MethodName>prepareToWrite</MethodName>
            <MethodComment>/** 
 * Initialize StoreFunc to write data.  This will be called during execution on the backend before the call to putNext.
 * @param writer RecordWriter to use.
 * @throws IOException if an exception occurs during initialization
 */
</MethodComment>
            <ReturnType>void</ReturnType>
            <ParameterList>
                <Parameter>
                    <ParamName>writer</ParamName>
                    <ParamType>RecordWriter</ParamType>
                </Parameter>
            </ParameterList>
            <ThrowExceptionList>
                <ExceptionType>IOException</ExceptionType>
            </ThrowExceptionList>
        </Method>
        <Method>
            <MethodName>putNext</MethodName>
            <MethodComment>/** 
 * Write a tuple to the data store.
 * @param t the tuple to store.
 * @throws IOException if an exception occurs during the write
 */
</MethodComment>
            <ReturnType>void</ReturnType>
            <ParameterList>
                <Parameter>
                    <ParamName>t</ParamName>
                    <ParamType>Tuple</ParamType>
                </Parameter>
            </ParameterList>
            <ThrowExceptionList>
                <ExceptionType>IOException</ExceptionType>
            </ThrowExceptionList>
        </Method>
        <Method>
            <MethodName>setStoreFuncUDFContextSignature</MethodName>
            <MethodComment>/** 
 * This method will be called by Pig both in the front end and back end to pass a unique signature to the  {@link StoreFunc} which it can use to storeinformation in the  {@link UDFContext} which it needs to store betweenvarious method invocations in the front end and back end. This method will be called before other methods in  {@link StoreFunc}.  This is necessary because in a Pig Latin script with multiple stores, the different instances of store functions need to be able to find their (and only their) data in the UDFContext object.  The default implementation is a no-op.
 * @param signature a unique signature to identify this StoreFunc
 */
</MethodComment>
            <ReturnType>void</ReturnType>
            <ParameterList>
                <Parameter>
                    <ParamName>signature</ParamName>
                    <ParamType>String</ParamType>
                </Parameter>
            </ParameterList>
            <InnerVarList>
                <InnerVar></InnerVar>
            </InnerVarList>
            <InnerMethodInvokeList>
                <InnerMethodInvoke></InnerMethodInvoke>
            </InnerMethodInvokeList>
            <ThrowExceptionList>
                <ExceptionType></ExceptionType>
            </ThrowExceptionList>
        </Method>
        <Method>
            <MethodName>cleanupOnFailure</MethodName>
            <MethodComment>/** 
 * This method will be called by Pig if the job which contains this store fails. Implementations can clean up output locations in this method to ensure that no incorrect/incomplete results are left in the output location. The default implementation  deletes the output location if it is a  {@link FileSystem} location.
 * @param location Location returned by{@link StoreFunc#relToAbsPathForStoreLocation(String,Path)}
 * @param job The {@link Job} object - this should be used only to obtaincluster properties through  {@link Job#getConfiguration()} and not to set/queryany runtime job information.
 */
</MethodComment>
            <ReturnType>void</ReturnType>
            <ParameterList>
                <Parameter>
                    <ParamName>location</ParamName>
                    <ParamType>String</ParamType>
                </Parameter>
                <Parameter>
                    <ParamName>job</ParamName>
                    <ParamType>Job</ParamType>
                </Parameter>
            </ParameterList>
            <InnerVarList>
                <InnerVar></InnerVar>
            </InnerVarList>
            <InnerMethodInvokeList>
                <InnerMethodInvoke>null;cleanupOnFailureImpl;[location, job]</InnerMethodInvoke>
            </InnerMethodInvokeList>
            <ThrowExceptionList>
                <ExceptionType>IOException</ExceptionType>
            </ThrowExceptionList>
        </Method>
        <Method>
            <MethodName>cleanupOnSuccess</MethodName>
            <MethodComment>/** 
 * This method will be called by Pig if the job which contains this store is successful, and some cleanup of intermediate resources is required. Implementations can clean up output locations in this method to ensure that no incorrect/incomplete results are left in the output location.
 * @param location Location returned by{@link StoreFunc#relToAbsPathForStoreLocation(String,Path)}
 * @param job The {@link Job} object - this should be used only to obtaincluster properties through  {@link Job#getConfiguration()} and not to set/queryany runtime job information.
 */
</MethodComment>
            <ReturnType>void</ReturnType>
            <ParameterList>
                <Parameter>
                    <ParamName>location</ParamName>
                    <ParamType>String</ParamType>
                </Parameter>
                <Parameter>
                    <ParamName>job</ParamName>
                    <ParamType>Job</ParamType>
                </Parameter>
            </ParameterList>
            <InnerVarList>
                <InnerVar></InnerVar>
            </InnerVarList>
            <InnerMethodInvokeList>
                <InnerMethodInvoke></InnerMethodInvoke>
            </InnerMethodInvokeList>
            <ThrowExceptionList>
                <ExceptionType>IOException</ExceptionType>
            </ThrowExceptionList>
        </Method>
        <Method>
            <MethodName>cleanupOnFailureImpl</MethodName>
            <MethodComment>/** 
 * Default implementation for  {@link #cleanupOnFailure(String,Job)}and  {@link #cleanupOnSuccess(String,Job)}.  This removes a file from HDFS.
 * @param location file name (or URI) of file to remove
 * @param job Hadoop job, used to access the appropriate file system.
 * @throws IOException
 */
</MethodComment>
            <ReturnType>void</ReturnType>
            <ParameterList>
                <Parameter>
                    <ParamName>location</ParamName>
                    <ParamType>String</ParamType>
                </Parameter>
                <Parameter>
                    <ParamName>job</ParamName>
                    <ParamType>Job</ParamType>
                </Parameter>
            </ParameterList>
            <InnerVarList>
                <InnerVar>Path [path=new Path(location)]</InnerVar>
                <InnerVar>FileSystem [fs=path.getFileSystem(job.getConfiguration())]</InnerVar>
            </InnerVarList>
            <InnerMethodInvokeList>
                <InnerMethodInvoke></InnerMethodInvoke>
            </InnerMethodInvokeList>
            <ThrowExceptionList>
                <ExceptionType>IOException</ExceptionType>
            </ThrowExceptionList>
        </Method>
        <Method>
            <MethodName>supportsParallelWriteToStoreLocation</MethodName>
            <MethodComment>/** 
 * DAG execution engines like Tez support optimizing union by writing to output location in parallel from tasks of different vertices. Commit is called once all the vertices in the union are complete. This eliminates need to have a separate phase to read data output from previous phases, union them and write out again. Enabling the union optimization requires the OutputFormat to 1) Support creation of different part file names for tasks of different vertices. Conflicting filenames can create data corruption and loss. For eg: If task 0 of vertex1 and vertex2 both create filename as part-r-00000, then one of the files will be overwritten when promoting from temporary to final location leading to data loss. FileOutputFormat has mapreduce.output.basename config which enables naming files differently in different vertices. Classes extending FileOutputFormat and those prefixing file names with mapreduce.output.basename value will not encounter conflict. Cases like HBaseStorage which write to key value store and do not produce files also should not face any conflict. 2) Support calling of commit once at the end takes care of promoting temporary files of the different vertices into the final location. For eg: FileOutputFormat commit algorithm handles promoting of files produced by tasks of different vertices into final output location without issues if there is no file name conflict. In cases like HBaseStorage, the TableOutputCommitter does nothing on commit. If custom OutputFormat used by the StoreFunc does not support the above two criteria, then false should be returned. Union optimization will be disabled for the StoreFunc. Default implementation returns null and in that case planner falls back to  {@link PigConfiguration.PIG_TEZ_OPT_UNION_SUPPORTED_STOREFUNCS} and{@link PigConfiguration.PIG_TEZ_OPT_UNION_UNSUPPORTED_STOREFUNCS}settings to determine if the StoreFunc supports it.
 */
</MethodComment>
            <ReturnType>Boolean</ReturnType>
            <ParameterList/>
            <InnerVarList>
                <InnerVar></InnerVar>
            </InnerVarList>
            <InnerMethodInvokeList>
                <InnerMethodInvoke></InnerMethodInvoke>
            </InnerMethodInvokeList>
            <ThrowExceptionList>
                <ExceptionType></ExceptionType>
            </ThrowExceptionList>
        </Method>
        <Method>
            <MethodName>warn</MethodName>
            <MethodComment>/** 
 * Issue a warning.  Warning messages are aggregated and reported to the user.
 * @param msg String message of the warning
 * @param warningEnum type of warning
 */
</MethodComment>
            <ReturnType>void</ReturnType>
            <ParameterList>
                <Parameter>
                    <ParamName>msg</ParamName>
                    <ParamType>String</ParamType>
                </Parameter>
                <Parameter>
                    <ParamName>warningEnum</ParamName>
                    <ParamType>Enum</ParamType>
                </Parameter>
            </ParameterList>
            <InnerVarList>
                <InnerVar></InnerVar>
            </InnerVarList>
            <InnerMethodInvokeList>
                <InnerMethodInvoke>PigHadoopLogger.getInstance();warn;[this, msg, warningEnum]</InnerMethodInvoke>
            </InnerMethodInvokeList>
            <ThrowExceptionList>
                <ExceptionType></ExceptionType>
            </ThrowExceptionList>
        </Method>
    </MethodList>
</Class>